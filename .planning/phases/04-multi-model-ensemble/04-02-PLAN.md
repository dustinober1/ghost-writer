---
phase: 04-multi-model-ensemble
plan: 02
type: execute
wave: 2
depends_on: [04-01]
files_modified:
  - backend/app/ml/ensemble/calibration.py
  - backend/app/ml/ensemble/ensemble_detector.py
  - backend/app/api/routes/ensemble.py
  - backend/app/models/schemas.py
  - backend/app/services/performance_monitor.py
autonomous: true

must_haves:
  truths:
    - "Ensemble probabilities are calibrated to prevent overconfidence"
    - "Calibration uses cross-validation to avoid data leakage"
    - "System tracks per-model accuracy metrics over time"
    - "API endpoint exposes model performance statistics"
    - "Weights update based on observed model performance"
  artifacts:
    - path: "backend/app/ml/ensemble/calibration.py"
      provides: "Probability calibration using CalibratedClassifierCV"
      min_lines: 60
      exports: ["CalibratedEnsemble", "fit_calibration", "calibrate_predict"]
    - path: "backend/app/services/performance_monitor.py"
      provides: "Model performance tracking and weight updates"
      min_lines: 80
      exports: ["PerformanceMonitor", "track_prediction", "update_weights", "get_model_stats"]
    - path: "backend/app/api/routes/ensemble.py"
      provides: "API endpoints for ensemble management"
      min_lines: 50
      exports: ["/api/ensemble/stats", "/api/ensemble/calibrate", "/api/ensemble/weights"]
  key_links:
    - from: "backend/app/ml/ensemble/ensemble_detector.py"
      to: "backend/app/ml/ensemble/calibration.py"
      via: "CalibratedClassifierCV wrapper around ensemble"
      pattern: "CalibratedClassifierCV"
    - from: "backend/app/api/routes/ensemble.py"
      to: "backend/app/services/performance_monitor.py"
      via: "PerformanceMonitor for stats and weight updates"
      pattern: "PerformanceMonitor"
    - from: "backend/app/services/analysis_service.py"
      to: "backend/app/services/performance_monitor.py"
      via: "Track predictions after each analysis"
      pattern: "track_prediction"
---

<objective>
Implement probability calibration and performance monitoring for the ensemble system, ensuring well-calibrated AI probabilities and dynamic weight adjustment based on model performance.

Purpose: Raw model outputs are often overconfident (probabilities clustered near 0 or 1). Calibration maps these to true probabilities, making the AI percentage meaningful to users. Performance tracking enables continuous improvement as the system learns which models perform better.

Output: Calibrated ensemble with monitored accuracy metrics, API endpoints for statistics, and dynamic weight adjustment based on observed performance.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/04-multi-model-ensemble/04-RESEARCH.md

@.planning/phases/04-multi-model-ensemble/04-01-PLAN.md
@backend/app/ml/ensemble/ensemble_detector.py
@backend/app/services/analysis_service.py
</context>

<tasks>

<task type="auto">
  <name>Implement probability calibration with CalibratedClassifierCV</name>
  <files>backend/app/ml/ensemble/calibration.py, backend/app/ml/ensemble/ensemble_detector.py</files>
  <action>
    Create backend/app/ml/ensemble/calibration.py:

    1. CalibratedEnsemble class:
       - Wraps VotingClassifier with sklearn.calibration.CalibratedClassifierCV
       - __init__(ensemble, method='sigmoid', cv=5):
         * ensemble: fitted VotingClassifier
         * method: 'sigmoid' (Platt scaling) for small datasets, 'isotonic' for larger
         * cv: number of cross-validation folds (default 5)
       - fit(X_calib, y_calib): fits calibration on separate dataset
       - predict_proba(X): returns calibrated probabilities
       - get_calibration_metrics(): returns Brier score, reliability diagram data

    2. fit_calibration(ensemble, X_calib, y_calib, method='sigmoid') function:
       - Wraps ensemble in CalibratedClassifierCV
       - Uses ensemble=True to create ensemble of calibrated classifiers
       - Fits on calibration data (not training data - prevents leakage)
       - Returns calibrated ensemble

    3. calibrate_predict(calibrated_ensemble, text) function:
       - Extracts features from text
       - Returns calibrated AI probability

    4. generate_calibration_dataset(n_samples=1000) helper:
       - Generates synthetic calibration data if real dataset unavailable
       - Mixes human and AI-like text patterns
       - Returns X, y for calibration

    Update backend/app/ml/ensemble/ensemble_detector.py:
    - Add optional use_calibration parameter to __init__
    - If calibration=True, wrap ensemble in CalibratedClassifierCV
    - Add calibrate(X_calib, y_calib) method
    - Add get_calibration_metrics() method returning Brier score

    IMPORTANT: Use CalibratedClassifierCV, NOT manual sigmoid fitting. It handles cross-validation correctly and supports both sigmoid and isotonic methods.

    Use method='sigmoid' (Platt scaling) as default - works better for small datasets (<1000 samples). Isotonic regression requires more data but can fit non-monotonic patterns.
  </action>
  <verify>
    python -c "from backend.app.ml.ensemble.calibration import CalibratedEnsemble, fit_calibration; print('Calibration module imported')"
  </verify>
  <done>
    CalibratedEnsemble class wraps VotingClassifier with calibration, fit_calibration() uses CalibratedClassifierCV with cross-validation, calibration metrics available
  </done>
</task>

<task type="auto">
  <name>Implement performance monitoring and dynamic weight updates</name>
  <files>backend/app/services/performance_monitor.py</files>
  <action>
    Create backend/app/services/performance_monitor.py:

    1. PerformanceMonitor class:
       - __init__(db_session): initializes with database session
       - Tracks per-model performance: stylometric, perplexity, contrastive, ensemble
       - Stores in ModelPerformance table or JSON file

    2. track_prediction(self, model_name: str, predicted_prob: float, actual_label: int, document_id: int):
       - Records prediction result for each model
       - actual_label: 0 = human, 1 = AI (from user feedback or ground truth)
       - Stores timestamp for time-based analysis
       - Updates running metrics: correct_count, total_count, accuracy

    3. update_weights(self) -> dict:
       - Calculates current accuracy for each model
       - Normalizes to sum to 1.0
       - Returns weights dict: {stylometric: 0.4, perplexity: 0.3, ...}
       - Handles cold start (no predictions yet) with default weights

    4. get_model_stats(self) -> dict:
       - Returns per-model statistics:
         * model_name: str
         * total_predictions: int
         * correct_predictions: int
         * accuracy: float
         * last_updated: datetime
         * avg_confidence: float (when prediction >0.7)

    5. calculate_brier_score(self, model_name: str) -> float:
       - Measures probability calibration quality
       - Lower is better (0 = perfect calibration)
       - Formula: mean((predicted_prob - actual_label)^2)

    6. get_reliability_data(self, model_name: str, n_bins: int = 10) -> dict:
       - Returns data for reliability diagram
       * Bins predicted probabilities into n_bins
       * For each bin: mean_predicted, mean_actual, count
       * Used to visualize calibration quality

    7. ModelPerformance database model (add to backend/app/models/database.py):
       - id, model_name, correct_count, total_count
       - accuracy (computed), last_updated
       - Optional: JSON field for per-metric tracking

    Use SQLAlchemy for persistence if DB available, otherwise JSON file fallback.

    Weight update strategy:
    - Minimum 100 predictions before updating weights (avoid noise)
    - Exponential moving average for smoothing: weight_new = 0.7 * weight_old + 0.3 * current_accuracy
    - Minimum weight of 0.1 per model (no model gets zeroed out)
  </action>
  <verify>
    python -c "from backend.app.services.performance_monitor import PerformanceMonitor; print('PerformanceMonitor imported')"
  </verify>
  <done>
    PerformanceMonitor tracks predictions, calculates accuracy per model, updates weights, returns stats dict, persists to database or JSON
  </done>
</task>

<task type="auto">
  <name>Create ensemble management API endpoints</name>
  <files>backend/app/api/routes/ensemble.py, backend/app/models/schemas.py</files>
  <action>
    Create backend/app/api/routes/ensemble.py:

    1. GET /api/ensemble/stats:
       - Returns ensemble performance statistics
       - Response schema:
         * model_stats: list of ModelStats (name, accuracy, total_predictions, etc.)
         * calibration_metrics: {brier_score: float, last_calibrated: datetime}
         * current_weights: dict[str, float]
         * ensemble_accuracy: float
       - Requires authentication (JWT or API key)

    2. POST /api/ensemble/calibrate:
       - Triggers recalibration of ensemble
       - Body: optional {method: 'sigmoid'|'isotonic', cv: int}
       - Runs calibration on stored calibration dataset
       - Returns: {status: 'calibrated', brier_score: float, timestamp: datetime}
       - Admin-only endpoint (requires is_admin=True)

    3. GET /api/ensemble/weights:
       - Returns current ensemble weights
       - Response: {stylometric: float, perplexity: float, contrastive: float}
       - No authentication required (read-only)

    4. PUT /api/ensemble/weights:
       - Manually override ensemble weights
       - Body: {stylometric: float, perplexity: float, contrastive: float}
       - Validates weights sum to 1.0, each weight >= 0.05
       - Admin-only endpoint

    Update backend/app/models/schemas.py:
    - Add ModelStats schema:
      * model_name: str
      * accuracy: float
      * total_predictions: int
      * correct_predictions: int
      * avg_confidence: float
      * last_updated: datetime

    - Add EnsembleStatsResponse schema:
      * model_stats: List[ModelStats]
      * calibration_metrics: CalibrationMetrics
      * current_weights: Dict[str, float]
      * ensemble_accuracy: float

    - Add CalibrationMetrics schema:
      * brier_score: float
      * last_calibrated: datetime
      * method: str ('sigmoid' or 'isotonic')

    - Add CalibrateRequest schema:
      * method: str = 'sigmoid'
      * cv: int = 5

    - Add CalibrateResponse schema:
      * status: str
      * brier_score: float
      * timestamp: datetime

    - Add UpdateWeightsRequest schema:
      * stylometric: float
      * perplexity: float
      * contrastive: float

    Register router in backend/app/main.py:
    - Include app.include_router(ensemble.router, prefix="/api/ensemble", tags=["ensemble"])

    All endpoints require authentication except GET /weights (read-only).
  </action>
  <verify>
    curl -X GET http://localhost:8000/api/ensemble/stats -H "Authorization: Bearer <token>" | jq '.model_stats'
  </verify>
  <done>
    Four API endpoints implemented, schemas defined, router registered, authentication enforced appropriately
  </done>
</task>

</tasks>

<verification>
1. CalibratedEnsemble wraps VotingClassifier with CalibratedClassifierCV
2. PerformanceMonitor tracks predictions and calculates accuracy
3. Weights update based on observed performance with minimum prediction threshold
4. API endpoints return ensemble statistics and allow calibration
5. Calibration metrics (Brier score, reliability data) available
6. Existing ensemble functionality unchanged (backward compatible)
</verification>

<success_criteria>
1. Ensemble probabilities are calibrated using cross-validation
2. Brier score tracked and exposed via API
3. Per-model accuracy statistics tracked over time
4. Weights update dynamically based on performance (with minimum threshold)
5. API endpoints provide ensemble transparency (stats, weights, calibration)
6. Admin can manually override weights if needed
7. Reliability diagram data available for calibration visualization
</success_criteria>

<output>
After completion, create `.planning/phases/04-multi-model-ensemble/04-02-SUMMARY.md`
</output>
