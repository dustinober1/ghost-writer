---
phase: 01-explainability
plan: 03
type: execute
wave: 3
depends_on: [01-01, 01-02]
files_modified:
  - backend/app/services/analysis_service.py
  - backend/app/models/schemas.py
  - frontend/src/components/HeatMap/HeatMap.tsx
autonomous: false
must_haves:
  truths:
    - "User can view clear, natural language explanations of why content was flagged (not just that it was flagged)"
    - "System generates document-level summary explaining overall AI likelihood"
    - "System generates sentence-level explanations when user clicks segments"
    - "Explanations reference specific feature values and patterns detected"
  artifacts:
    - path: "backend/app/services/analysis_service.py"
      provides: "Natural language explanation generation"
      exports: ["generate_document_explanation", "generate_sentence_explanation"]
    - path: "backend/app/models/schemas.py"
      provides: "Enhanced response schemas with explanations"
      contains: "document_explanation, sentence_explanation fields"
    - path: "frontend/src/components/HeatMap/HeatMap.tsx"
      provides: "Explanation display UI"
      contains: "DocumentExplanationCard, SentenceExplanationCard"
  key_links:
    - from: "backend/app/services/analysis_service.py"
      to: "feature_attribution"
      via: "Use feature attribution to generate explanations"
      pattern: "generate_.*explanation.*feature_attribution"
    - from: "frontend/src/components/HeatMap/HeatMap.tsx"
      to: "analysisResult.document_explanation"
      via: "Display document summary at top of results"
      pattern: "document_explanation.*Card"
---

<objective>
Generate clear, natural language explanations at document and sentence level that tell users WHY their content was flagged as AI-generated.

Purpose: Move beyond raw scores and feature lists to provide human-readable explanations that reference specific patterns, feature values, and AI indicators detected in the text.

Output: Two-tier explanation system (document-level summary + sentence-level details) that transforms technical analysis into accessible insights.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md

@backend/app/services/analysis_service.py
@backend/app/models/schemas.py
@frontend/src/components/HeatMap/HeatMap.tsx

# Reference 01-01-SUMMARY.md and 01-02-SUMMARY.md for confidence and feature attribution context
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement document-level explanation generation</name>
  <files>backend/app/services/analysis_service.py</files>
  <action>
    Add document-level explanation generation:

    1. Create generate_document_explanation(result: Dict) -> str:
       - Takes full analysis result (segments, overall probability, feature distributions)
       - Generates 2-3 sentence summary explaining overall assessment

    2. Explanation logic (template-based, no LLM):
       - If overall_ai_probability > 0.7:
         "This document shows strong indicators of AI-generated content. Key patterns include [feature_pattern_summary]. Approximately [high_count] of [total] sentences are flagged as high-confidence AI-generated."

       - If overall_ai_probability 0.4-0.7:
         "This document contains mixed signals - some sections appear AI-generated while others seem human-written. [feature_pattern_summary]. Overall, [high_count] of [total] sentences show strong AI patterns."

       - If overall_ai_probability < 0.4:
         "This document primarily shows human-like writing patterns. [feature_pattern_summary]. Only [high_count] of [total] sentences are flagged as potentially AI-generated."

    3. Feature pattern summary generation:
       - Analyze which features are most important across high-confidence segments
       - If low_burstiness dominant: "consistent sentence lengths throughout"
       - If low_perplexity dominant: "predictable word choices and common phrases"
       - If low_unique_words: "repetitive vocabulary"
       - Combine 2-3 patterns into readable summary

    4. Include in analyze_text() result:
       - Add "document_explanation" field to result dictionary
       - Call generate_document_explanation() before returning

    DO NOT use Ollama/LLM for explanations - use template-based approach.
  </action>
  <verify>
    Run: python -c "from backend.app.services.analysis_service import get_analysis_service; service = get_analysis_service(); result = service.analyze_text('Test text here.', 'sentence'); print('Explanation:', result.get('document_explanation', 'NOT FOUND'))"
    Expected: result contains document_explanation with readable summary text
  </verify>
  <done>
    - generate_document_explanation() function exists
    - Returns 2-3 sentence summary with specific references
    - Mentions high-confidence sentence count and total
    - References specific feature patterns detected
    - document_explanation included in analyze_text() result
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement sentence-level explanation generation</name>
  <files>backend/app/services/analysis_service.py</files>
  <action>
    Add sentence-level explanation generation:

    1. Create generate_sentence_explanation(segment: Dict) -> str:
       - Takes single segment with ai_probability, confidence_level, feature_attribution
       - Generates 1-2 sentence explanation specific to this sentence

    2. Explanation logic by confidence level:
       - HIGH confidence (>0.7):
         "This sentence is flagged as highly likely to be AI-generated. Primary indicators: [top_2_features_with_values]. The sentence shows [pattern_description]."

       - MEDIUM confidence (0.4-0.7):
         "This sentence shows some AI-like patterns but is less certain. Contributing factors: [top_2_features_with_values]. May benefit from [suggestion_for_humanizing]."

       - LOW confidence (<0.4):
         "This sentence appears primarily human-written. Human-like indicators: [top_2_human_features]. No strong AI patterns detected."

    3. Pattern descriptions based on top features:
       - Low burstiness + low perplexity: "uniform sentence structure and predictable vocabulary"
       - High rare words + high perplexity: "varied vocabulary and complex word choices"
       - Low unique words: "repetitive word patterns"
       - etc.

    4. Suggestions for humanizing (medium confidence only):
       - If low burstiness: "varying sentence length"
       - If low unique words: "using more diverse vocabulary"
       - If high coherence: "adding more topic transitions"

    5. Integrate into analyze_text():
       - Call generate_sentence_explanation() for each segment
       - Add "sentence_explanation" to segment_results

    DO NOT use LLM - use conditional templates based on feature values.
  </action>
  <verify>
    Run: curl -X POST http://localhost:8000/api/analysis/analyze -H "Content-Type: application/json" -d '{"text": "This is a test.", "granularity": "sentence"}' | jq '.heat_map_data.segments[0].sentence_explanation'
    Expected: Each segment has sentence_explanation field with specific, readable explanation
  </verify>
  <done>
    - generate_sentence_explanation() function exists
    - Returns 1-2 sentence explanation specific to segment
    - References top 2 features and their values
    - Includes pattern description or suggestion
    - sentence_explanation included in segment_results
  </done>
</task>

<task type="auto">
  <name>Task 3: Update schemas and API for explanations</name>
  <files>backend/app/models/schemas.py, backend/app/api/routes/analysis.py</files>
  <action>
    1. In backend/app/models/schemas.py:
       - Extend HeatMapData:
         ```python
         class HeatMapData(BaseModel):
           segments: List[TextSegment]
           overall_ai_probability: float
           confidence_distribution: Optional[Dict[str, int]] = None
           document_explanation: Optional[str] = None
         ```

       - Extend TextSegment:
         ```python
         class TextSegment(BaseModel):
           text: str
           ai_probability: float
           confidence_level: Optional[ConfidenceLevel] = None
           start_index: int
           end_index: int
           feature_attribution: Optional[List[FeatureAttribution]] = None
           sentence_explanation: Optional[str] = None
         ```

    2. In backend/app/api/routes/analysis.py:
       - Ensure document_explanation flows through to response
       - Ensure sentence_explanation flows through for each segment
       - No code changes needed if schema extensions are optional - Pydantic handles automatically

    3. Verify response includes both explanation levels.
  </action>
  <verify>
    Run: curl -X POST http://localhost:8000/api/analysis/analyze | jq '{document: .heat_map_data.document_explanation, sentence: .heat_map_data.segments[0].sentence_explanation}'
    Expected: Both document_explanation and sentence_explanation are present and non-null
  </verify>
  <done>
    - HeatMapData includes document_explanation field
    - TextSegment includes sentence_explanation field
    - API responses include both explanation types
    - Schema validation passes with optional fields
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Backend explanation generation system</what-built>
  <how-to-verify>
    1. Start backend: docker-compose up backend
    2. Submit sample text for analysis
    3. Check API response in DevTools:
       - heat_map_data.document_explanation exists and is readable
       - segments[].sentence_explanation exists for each segment
       - Explanations reference specific features and patterns
       - Different confidence levels have different explanation styles

    4. Test with varied texts:
       - AI-generated text: Should explain "strong indicators" and specific AI patterns
       - Human text: Should explain "human-like patterns" and which features show this
       - Mixed text: Should explain "mixed signals" and breakdown

    Expected: Explanations are specific, readable, and reference actual detected patterns
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues with explanation quality/accuracy</resume-signal>
</task>

<task type="auto">
  <name>Task 4: Create frontend explanation display UI</name>
  <files>frontend/src/components/HeatMap/HeatMap.tsx</files>
  <action>
    Add explanation display to HeatMap component:

    1. Update interfaces:
       ```typescript
       interface HeatMapData {
         segments: TextSegment[];
         overall_ai_probability: number;
         document_explanation?: string;
       }
       interface TextSegment {
         // ... existing
         sentence_explanation?: string;
       }
       ```

    2. Add Document Explanation Card (at top of main content, below Overall AI Probability card):
       - Title: "What This Means"
       - Icon: Info or Lightbulb
       - Content: document_explanation text
       - Styling: Callout/info box style, subtle background
       - Placement: Prominent but below main score

    3. Enhance Segment Details sidebar:
       - Add "Explanation" section (below "Why This Flag?" feature attribution)
       - Title: "In Plain English"
       - Content: selectedSegment.sentence_explanation
       - Styling: Readable paragraph, good spacing, distinct from feature list
       - Placement: Near top of sidebar, visible immediately when segment selected

    4. Visual hierarchy:
       - Document explanation: Prominent, near top of page, sets context
       - Sentence explanation: In sidebar, context-specific, helps understand selected segment

    5. Handle missing explanations:
       - If document_explanation is null: Don't show card
       - If sentence_explanation is null: Show "No explanation available"

    6. Typography:
       - Use readable font size (14-16px)
       - Good line height (1.5-1.6)
       - Clear contrast with background

    Ensure explanations don't clutter UI - they enhance understanding without overwhelming.
  </action>
  <verify>
    Run: npm run build (no TypeScript errors)
    Run: npm run test (if tests exist, ensure they pass)
    Check: HeatMap.tsx renders document explanation card
    Check: HeatMap.tsx renders sentence explanation in sidebar
  </verify>
  <done>
    - Document explanation card displays below main score
    - Sentence explanation appears in sidebar when segment selected
    - Both are styled clearly and readable
    - Missing explanations handled gracefully
    - UI doesn't feel cluttered
    - Explanations enhance rather than overwhelm
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete explanation UI</what-built>
  <how-to-verify>
    1. Start full application: docker-compose up
    2. Submit text for analysis and view results
    3. Verify document-level explanation:
       - "What This Means" card appears near top
       - Text is readable and explains overall assessment
       - References specific patterns and sentence counts
       - Helps user understand document-level verdict

    4. Verify sentence-level explanation:
       - Click on any segment
       - Sidebar shows "In Plain English" section
       - Text explains WHY that specific sentence was flagged
       - References features mentioned in "Why This Flag?" section
       - Different confidence levels have different explanation styles

    5. Test usability:
       - Read explanations without technical background
       - Check if explanations clarify rather than confuse
       - Verify explanations match confidence scores and feature attribution

    Expected: Users can understand AI detection results without needing ML expertise
  </how-to-verify>
  <resume-signal>Type "approved" or describe UX issues with explanation display</resume-signal>
</task>

</tasks>

<verification>
1. Backend generates document-level explanation summarizing overall assessment
2. Backend generates sentence-level explanation for each segment
3. Explanations reference specific features and patterns detected
4. Frontend displays document explanation prominently
5. Frontend displays sentence explanation in segment details sidebar
6. Explanations are readable and understandable without technical background
7. User understands WHY content was flagged, not just THAT it was flagged
</verification>

<success_criteria>
- [ ] generate_document_explanation() implemented and called
- [ ] generate_sentence_explanation() implemented and called
- [ ] HeatMapData schema includes document_explanation
- [ ] TextSegment schema includes sentence_explanation
- [ ] Document explanation card displays in UI
- [ ] Sentence explanation displays in sidebar
- [ ] Explanations reference specific features and patterns
- [ ] Explanations are readable and helpful
</success_criteria>

<output>
After completion, create `.planning/phases/01-explainability/01-03-SUMMARY.md`
</output>
