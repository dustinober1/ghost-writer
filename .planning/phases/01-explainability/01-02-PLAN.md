---
phase: 01-explainability
plan: 02
type: execute
wave: 2
depends_on: [01-01]
files_modified:
  - backend/app/ml/feature_extraction.py
  - backend/app/services/analysis_service.py
  - backend/app/models/schemas.py
  - frontend/src/components/HeatMap/HeatMap.tsx
autonomous: false
must_haves:
  truths:
    - "User can view feature attribution showing which stylometric features contributed to AI detection for each sentence"
    - "Backend returns top 3-5 contributing features with normalized importance scores (0-1)"
    - "Frontend displays feature attribution in segment details sidebar"
    - "User understands WHICH features triggered the AI flag (e.g., 'Low burstiness', 'High perplexity')"
  artifacts:
    - path: "backend/app/ml/feature_extraction.py"
      provides: "Feature importance calculation for individual sentences"
      exports: ["calculate_feature_importance"]
    - path: "backend/app/models/schemas.py"
      provides: "FeatureAttribution schema for top contributing features"
      contains: "class FeatureAttribution"
    - path: "frontend/src/components/HeatMap/HeatMap.tsx"
      provides: "Feature attribution display in segment sidebar"
      contains: "renderFeatureAttribution"
  key_links:
    - from: "backend/app/services/analysis_service.py"
      to: "backend/app/ml/feature_extraction.py"
      via: "calculate_feature_importance call for each segment"
      pattern: "calculate_feature_importance.*segment"
    - from: "frontend/src/components/HeatMap/HeatMap.tsx"
      to: "selectedSegment.feature_attribution"
      via: "Display top features with importance bars"
      pattern: "feature_attribution.*map"
---

<objective>
Add per-sentence feature attribution showing which stylometric features contributed most to the AI detection score.

Purpose: Users need to understand WHY specific sentences were flagged by seeing which features (burstiness, perplexity, rare words, etc.) drove the AI probability score.

Output: Feature attribution system showing top 3-5 contributing features per sentence with normalized importance scores and human-readable explanations.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md

@backend/app/ml/feature_extraction.py
@backend/app/services/analysis_service.py
@backend/app/models/schemas.py
@frontend/src/components/HeatMap/HeatMap.tsx

# Reference 01-01-SUMMARY.md when available for confidence system context
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement feature importance calculation</name>
  <files>backend/app/ml/feature_extraction.py</files>
  <action>
    Add feature importance calculation for individual sentences:

    1. Create calculate_feature_importance(text: str, ai_probability: float) -> Dict[str, float]:
       - Extract all 27 features for the sentence using extract_all_features()
       - Normalize each feature to [0, 1] range using known min/max from training data
       - Calculate importance score for each feature based on deviation from "human-like" baseline
       - Use existing feature extraction functions - no new ML model needed

    2. Define human-like baselines (from research/training):
       - Burstiness: >0.5 is human-like, <0.3 is AI-like
       - Perplexity: >50 is human-like, <30 is AI-like
       - Rare word ratio: >0.15 is human-like, <0.05 is AI-like
       - etc. (use existing feature_extraction.py knowledge)

    3. Importance formula:
       - For each feature, calculate absolute deviation from human baseline
       - Normalize by baseline range to get [0, 1] importance
       - Higher deviation = more important for AI detection

    4. Return sorted dict of {feature_name: importance_score} top 5-10 features

    DO NOT use SHAP or complex XAI libraries - use heuristic-based importance calculation.
    DO NOT modify existing feature extraction functions.
  </action>
  <verify>
    Run: python -c "from backend.app.ml.feature_extraction import calculate_feature_importance; result = calculate_feature_importance('This is a test sentence with some words.', 0.75); print(f'Top features: {list(result.keys())[:3]}')"
    Expected: Returns dict with feature names and importance scores (0-1 range)
  </verify>
  <done>
    - calculate_feature_importance() function exists
    - Returns dict of feature_name -> importance_score
    - Importance scores are normalized to [0, 1]
    - Top features correlate with AI probability (high AI prob = AI-like features important)
  </done>
</task>

<task type="auto">
  <name>Task 2: Add feature attribution schemas and service integration</name>
  <files>backend/app/models/schemas.py, backend/app/services/analysis_service.py</files>
  <action>
    1. In backend/app/models/schemas.py:
       - Add FeatureAttribution schema:
         ```python
         class FeatureAttribution(BaseModel):
           feature_name: str
           importance: float  # 0-1 normalized
           interpretation: str  # Human-readable explanation
         ```
       - Extend TextSegment to include feature_attribution: List[FeatureAttribution]

    2. In backend/app/services/analysis_service.py:
       - Import calculate_feature_importance
       - In analyze_text(), for each segment:
         - Call calculate_feature_importance(segment, ai_probability)
         - Get top 5 features by importance score
         - Generate interpretation text for each feature:
           - "Low burstiness (0.2) - consistent sentence length suggests AI"
           - "High perplexity (75.3) - unpredictable vocabulary suggests human"
         - Add feature_attribution list to segment_results

    3. Feature interpretation templates (implement as helper function):
       - burstiness: "Low/X - consistent/varied sentence lengths"
       - perplexity: "Low/X - predictable/unpredictable word patterns"
       - rare_word_ratio: "Low/X - common/rare vocabulary"
       - unique_word_ratio: "Low/X - repetitive/diverse word choice"
       - etc.

       Use feature value to determine "Low" vs "High" vs "Medium" in interpretation.
  </action>
  <verify>
    Run: curl -X POST http://localhost:8000/api/analysis/analyze -H "Content-Type: application/json" -d '{"text": "This is a test.", "granularity": "sentence"}' | jq '.heat_map_data.segments[0].feature_attribution'
    Expected: segments[].feature_attribution is array of 5 objects with feature_name, importance, interpretation
  </verify>
  <done>
    - FeatureAttribution schema defined with feature_name, importance, interpretation
    - TextSegment includes feature_attribution field
    - analyze_text() returns top 5 features per segment
    - Interpretation text is human-readable and specific to feature value
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Backend feature attribution system</what-built>
  <how-to-verify>
    1. Start backend: docker-compose up backend
    2. Submit analysis request via API or web UI
    3. Check API response in DevTools:
       - segments[].feature_attribution exists
       - Each feature has: feature_name, importance (0-1), interpretation text
       - Top 5 features are returned (not all 27)
       - Interpretation makes sense (e.g., "Low burstiness - consistent sentence lengths")

    4. Test with different sentences:
       - AI-generated sentence: Should see "Low burstiness", "Low perplexity" in top features
       - Human-written sentence: Should see "High burstiness", "Rare words" in top features

    Expected: Feature attribution explains WHY the sentence got its AI probability score
  </how-to-verify>
  <resume-signal>Type "approved" or describe issues with feature attribution calculation/interpretation</resume-signal>
</task>

<task type="auto">
  <name>Task 3: Create frontend feature attribution visualization</name>
  <files>frontend/src/components/HeatMap/HeatMap.tsx</files>
  <action>
    Add feature attribution display to segment details sidebar:

    1. Update TextSegment interface:
       ```typescript
       interface FeatureAttribution {
         feature_name: string;
         importance: number;
         interpretation: string;
       }
       interface TextSegment {
         // ... existing fields
         feature_attribution?: FeatureAttribution[];
       }
       ```

    2. In Segment Details sidebar (selectedSegment section), add new card:
       - Title: "Why This Flag?"
       - Subtitle: "Top contributing features"
       - For each feature in feature_attribution:
         - Display feature_name (e.g., "Burstiness")
         - Show importance as horizontal bar (0-100% width)
         - Color-code bar by importance (red=high, yellow=medium, green=low)
         - Show interpretation text below bar
         - Display raw value if available (e.g., "0.23")

    3. Visual design:
       - Stack features vertically (most important at top)
       - Use progress bars for importance visualization
       - Keep interpretation text concise (1-2 lines)
       - Add tooltip on feature_name with technical explanation

    4. If feature_attribution is missing/null, show:
       - "Feature attribution not available for this segment"
       - Or hide the section entirely

    5. Accessibility:
       - Use aria-label on bars: "Burstiness importance: 85%"
       - Ensure color contrast on bars
       - Allow keyboard navigation through features

    Place this card BELOW "Segment Details" but ABOVE "Statistics" in sidebar.
  </action>
  <verify>
    Run: npm run build (frontend compiles without TypeScript errors)
    Check: HeatMap.tsx has FeatureAttribution interface
    Check: selectedSegment section renders feature attribution list
  </verify>
  <done>
    - Feature attribution card displays in sidebar when segment selected
    - Top 5 features shown with importance bars
    - Interpretation text is readable and informative
    - Visual hierarchy is clear (most important features at top)
    - Graceful handling when feature_attribution unavailable
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete feature attribution UI</what-built>
  <how-to-verify>
    1. Start full application: docker-compose up
    2. Navigate to /analyze and submit text
    3. On HeatMap results page:
       - Click on a segment with HIGH AI probability
       - Verify sidebar shows "Why This Flag?" section
       - Check that top 5 features are listed
       - Verify importance bars are visible and proportionally sized
       - Read interpretation text - should make sense for the feature

    4. Test different segments:
       - HIGH confidence segment: Should see AI-like features (low burstiness, etc.)
       - LOW confidence segment: Should see human-like features (high burstiness, etc.)

    5. Test edge cases:
       - Very short segment (< 3 words): Should still show attribution
       - Segment with no attribution: Should handle gracefully

    Expected: User can immediately see which features drove the AI flag for any sentence
  </how-to-verify>
  <resume-signal>Type "approved" or describe UX issues with feature attribution display</resume-signal>
</task>

</tasks>

<verification>
1. Backend calculates feature importance for each sentence segment
2. Top 5 features returned with importance scores (0-1) and interpretation
3. Frontend displays feature attribution in segment details sidebar
4. Importance bars visualize relative contribution of each feature
5. Interpretation text explains feature value in plain language
6. User understands WHY specific sentences were flagged
</verification>

<success_criteria>
- [ ] calculate_feature_importance() function implemented
- [ ] FeatureAttribution schema defined
- [ ] TextSegment includes feature_attribution field
- [ ] analyze_text() returns top 5 features per segment
- [ ] Frontend displays "Why This Flag?" section in sidebar
- [ ] Importance bars show relative feature contribution
- [ ] Interpretation text is human-readable and accurate
- [ ] User can identify which features triggered AI detection
</success_criteria>

<output>
After completion, create `.planning/phases/01-explainability/01-02-SUMMARY.md`
</output>
