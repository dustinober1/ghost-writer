---
phase: 02-batch-analysis
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/models/database.py
  - backend/app/models/schemas.py
  - backend/alembic/versions/002_add_batch_analysis_tables.py
  - backend/tests/test_batch_analysis_models.py
autonomous: true
must_haves:
  truths:
    - "BatchAnalysisJob and BatchDocument models exist with required fields"
    - "Pydantic schemas serialize correctly for API responses"
    - "Alembic migration creates new tables and indexes"
    - "Tests validate model relationships and default values"
  artifacts:
    - path: "backend/app/models/database.py"
      provides: "BatchAnalysisJob and BatchDocument ORM models"
      contains: "class BatchAnalysisJob", "class BatchDocument"
    - path: "backend/app/models/schemas.py"
      provides: "Batch upload, status, and results schemas"
      contains: "BatchJobStatus", "BatchUploadResponse", "BatchResultsResponse"
    - path: "backend/alembic/versions/002_add_batch_analysis_tables.py"
      provides: "Database migration for batch tables"
      contains: "batch_analysis_jobs", "batch_documents"
    - path: "backend/tests/test_batch_analysis_models.py"
      provides: "Model relationship and default value tests"
      min_lines: 30
  key_links:
    - from: "backend/app/models/database.py"
      to: "backend/alembic/versions/002_add_batch_analysis_tables.py"
      via: "SQLAlchemy model definitions drive migration schema"
      pattern: "class (BatchAnalysisJob|BatchDocument)"
    - from: "backend/app/models/schemas.py"
      to: "backend/app/models/database.py"
      via: "Schema fields align with ORM model columns"
      pattern: "BatchAnalysisJob|BatchDocument"
---

<objective>
Define the batch analysis data model and schemas for jobs, documents, clusters, and stored results.

Purpose: Provide durable storage and typed responses for batch processing, clustering, and exports.
Output: Database models, Pydantic schemas, and a migration for new batch tables.
</objective>

<execution_context>
@~/.config/opencode/get-shit-done/workflows/execute-plan.md
@~/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-batch-analysis/02-CONTEXT.md
@.planning/codebase/ARCHITECTURE.md
@.planning/codebase/CONVENTIONS.md
@.planning/codebase/STRUCTURE.md
@backend/app/models/database.py
@backend/app/models/schemas.py
@backend/alembic/versions/001_add_auth_features.py
</context>

<tasks>
<task type="auto">
  <name>Task 1: Add batch job and document ORM models</name>
  <files>backend/app/models/database.py</files>
  <action>Create SQLAlchemy models for BatchAnalysisJob and BatchDocument. Include fields: status (string enum), total_documents, processed_documents, granularity, created_at, started_at, completed_at, error_message, similarity_matrix (JSON), clusters (JSON). For documents include: job_id FK, filename, source_type, text_content, word_count, status, ai_probability, confidence_distribution (JSON), heat_map_data (JSON), embedding (JSON list), cluster_id, error_message, created_at. Add relationships: User.batch_jobs, BatchAnalysisJob.documents with cascade. Avoid storing derived UI-only fields; only persist data used for API responses and exports.</action>
  <verify>python -c "from app.models.database import BatchAnalysisJob, BatchDocument; print(BatchAnalysisJob.__tablename__, BatchDocument.__tablename__)"</verify>
  <done>BatchAnalysisJob and BatchDocument are defined with required columns and relationships, and can be imported without errors.</done>
</task>

<task type="auto">
  <name>Task 2: Add batch analysis schemas and status enums</name>
  <files>backend/app/models/schemas.py</files>
  <action>Add BatchJobStatus and BatchDocumentStatus enums (PENDING, PROCESSING, COMPLETED, FAILED). Create Pydantic schemas: BatchUploadResponse (job_id, status), BatchJobStatusResponse (job fields + progress), BatchDocumentSummary (id, filename, word_count, ai_probability, confidence_level, cluster_id, status), BatchClusterSummary (cluster_id, document_count, avg_ai_probability), BatchResultsResponse (job + documents + clusters + similarity_matrix). Keep field names snake_case to align with backend conventions and return types JSON-serializable. Avoid embedding large raw text in list responses; include text only in detailed document responses.</action>
  <verify>python -c "from app.models.schemas import BatchJobStatus, BatchUploadResponse; print(BatchJobStatus.PENDING, BatchUploadResponse.model_fields.keys())"</verify>
  <done>Schemas and enums serialize correctly and cover upload, status, and results payloads.</done>
</task>

<task type="auto">
  <name>Task 3: Create alembic migration and model tests</name>
  <files>backend/alembic/versions/002_add_batch_analysis_tables.py, backend/tests/test_batch_analysis_models.py</files>
  <action>Write alembic migration to create batch_analysis_jobs and batch_documents tables with indexes on user_id/job_id and status fields. Add pytest coverage validating model relationships and default values (e.g., status defaults to PENDING, processed_documents defaults to 0). Use existing database fixtures and avoid hitting external services.</action>
  <verify>cd backend && pytest tests/test_batch_analysis_models.py</verify>
  <done>Migration creates both tables and model tests pass with 100% coverage for new lines.</done>
</task>
</tasks>

<verification>
Before declaring plan complete:
- [ ] cd backend && pytest tests/test_batch_analysis_models.py
</verification>

<success_criteria>
- BatchAnalysisJob and BatchDocument models exist with relationships and status fields
- Schemas cover upload, status, and results payloads without raw text in list responses
- Alembic migration creates new tables and indexes
- Tests pass and maintain backend coverage expectations
</success_criteria>

<output>
After completion, create `.planning/phases/02-batch-analysis/02-01-SUMMARY.md`
</output>
